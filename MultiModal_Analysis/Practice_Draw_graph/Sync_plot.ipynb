{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synchrony plot에 Annotation한 정보를 그려넣기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.signal import butter, filtfilt\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def directional_agreement(signal1, signal2):\n",
    "    # Normalize the signals\n",
    "    signal1_norm = (signal1 - np.mean(signal1)) / np.std(signal1)\n",
    "    signal2_norm = (signal2 - np.mean(signal2)) / np.std(signal2)\n",
    "    \n",
    "    # Compute the dot product\n",
    "    dot_product = np.dot(signal1_norm, signal2_norm)\n",
    "    \n",
    "    # Compute the directional agreement\n",
    "    directional_agreement = dot_product / len(signal1)\n",
    "    \n",
    "    return directional_agreement\n",
    "\n",
    "# 로우패스 필터 설계\n",
    "def butter_lowpass(cutoff, fs, order=5):\n",
    "    nyquist = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyquist\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    return b, a\n",
    "\n",
    "def lowpass_filter(data, cutoff, fs, order=5):\n",
    "    b, a = butter_lowpass(cutoff, fs, order=order)\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "def cross_correlation(signal1, signal2):\n",
    "    # Time lag range (-len(signal1)+1, len(signal1)-1)\n",
    "    time_lags = np.arange(-len(signal1) + 1, len(signal1))\n",
    "    # Compute cross-correlation for each time lag\n",
    "    cross_corr = [np.correlate(signal1, np.roll(signal2, shift), mode='valid')[0] for shift in time_lags]\n",
    "    \n",
    "    return np.array(cross_corr), time_lags\n",
    "\n",
    "# 1은 완벽한 양의 상관관계를 나타냄. -1은 음의 상관 관계를 나타냄. 0은 상관관계가 없음. \n",
    "\n",
    "def pearson_cross_correlation(signal1, signal2):\n",
    "    # Subtract the mean\n",
    "    signal1_mean = np.mean(signal1)\n",
    "    signal2_mean = np.mean(signal2)\n",
    "    \n",
    "    signal1_adjusted = signal1 - signal1_mean\n",
    "    signal2_adjusted = signal2 - signal2_mean\n",
    "    \n",
    "    # Calculate Pearson correlation coefficient\n",
    "    numerator = np.sum(signal1_adjusted * signal2_adjusted)\n",
    "    denominator = np.sqrt(np.sum(signal1_adjusted ** 2) * np.sum(signal2_adjusted ** 2))\n",
    "    \n",
    "    pearson_corr = numerator / denominator\n",
    "    \n",
    "    return pearson_corr\n",
    "\n",
    "def rolling_window_correlation(signal1, signal2, window_size):\n",
    "    num_samples = len(signal1)\n",
    "    correlations = []\n",
    "\n",
    "    for i in tqdm(range(num_samples - window_size + 1), desc=\"Calculating Rolling Window Correlation\"):\n",
    "        window1 = signal1[i : i + window_size]\n",
    "        window2 = signal2[i : i + window_size]\n",
    "        \n",
    "        # Calculate Pearson correlation coefficient for the current window\n",
    "        correlation = np.corrcoef(window1, window2)[0, 1]\n",
    "        correlations.append(correlation)\n",
    "\n",
    "    return np.array(correlations)\n",
    "\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band', analog=False)\n",
    "    return b, a\n",
    "\n",
    "def bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "def zscore_signal(signal):\n",
    "    signal_mean = np.mean(signal)\n",
    "    signal_std = np.std(signal)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    if signal_std == 0:\n",
    "        signal_normalized = np.zeros_like(signal)\n",
    "        #print(signal_normalized)\n",
    "    else:\n",
    "        signal_normalized = (signal - signal_mean) / signal_std\n",
    "        #print(signal_normalized)\n",
    "    \n",
    "    # # Subtract the mean and divide by the standard deviation\n",
    "    # signal_normalized = (signal - signal_mean) / signal_std\n",
    "    \n",
    "    return signal_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from CSV files || 입력받으려는 데이터의 경우, Head Rotation 의 (x,y,z 값과 각 축의 변화량 값, lip_distance 값)이 있는 것. \n",
    "# 얼굴 움직임 값을 계산하기 위해서 불러오기 위한 코드 부분. \n",
    "\n",
    "# Load the uploaded Excel file\n",
    "file_path = 'C:/Users/user/Desktop/Group_Lean_time_3.xlsx'\n",
    "xls = pd.ExcelFile(file_path)\n",
    "\n",
    "# Load the 'A_group' sheet into a DataFrame\n",
    "a_group_df = pd.read_excel(file_path, sheet_name='B_group_2W_S1') # B_group_S1, B_group_S2, B_group_2W_S1, B_group_2W_S2, G_group_S1, G_group_S2, G_group_2W_S1, G_group_2W_S2\n",
    "\n",
    "# Extracting the time information from the relevant column\n",
    "time_data = a_group_df['고개 위아래 끄덕임 (2명)'][1:]  # Skipping the first two rows which are headers\n",
    "print(time_data)\n",
    "# NaN 값이 있는 구간은 없애주기.\n",
    "time_data = time_data.dropna()\n",
    "#print(time_data)\n",
    "time_data = time_data.astype(str)\n",
    "\n",
    "# Converting time data to seconds\n",
    "def time_to_seconds(time_str):\n",
    "    h, m, s = map(int, time_str.split(':'))\n",
    "    # 시간, 분, 초 \n",
    "    #total_seconds = h * 3600 + m * 60 + s\n",
    "    \n",
    "    return h * 3600 + m * 60 + s\n",
    "    # 전체 시간에서 20분을 뻄. \n",
    "    if total_seconds > 1200:  # Only subtract 20 minutes if total time is more than 20 minutes\n",
    "        adjusted_seconds = total_seconds - 1200\n",
    "    else:\n",
    "        adjusted_seconds = total_seconds\n",
    "    return adjusted_seconds\n",
    "\n",
    "group_times_seconds_adjusted = time_data.apply(time_to_seconds).reset_index(drop=True)\n",
    "#group_times_seconds = time_data.apply(time_to_seconds).reset_index(drop=True)\n",
    "print(group_times_seconds_adjusted)\n",
    "\n",
    "\n",
    "# Extracting the time information from the '고개 위아래 끄덕임 (3명)' column\n",
    "time_data_s2_3 = a_group_df['고개 위아래 끄덕임 (3명)'][1:]  # Skipping the first two rows which are headers\n",
    "time_data_s2_3 = time_data_s2_3.dropna()\n",
    "time_data_s2_3 = time_data_s2_3.astype(str)\n",
    "\n",
    "# Converting time data to seconds and subtracting 20 minutes for '고개 위아래 끄덕임 (3명)'\n",
    "group_times_seconds_adjusted_3 = time_data_s2_3.apply(time_to_seconds).reset_index(drop=True)\n",
    "print(group_times_seconds_adjusted_3)\n",
    "\n",
    "csv_files = [\n",
    "    f'D:/MultiModal/Data/Data_PreProcessing/Head_Rotation_Mouse/B_group/Face_2W_B1_S1.csv',\n",
    "    f'D:/MultiModal/Data/Data_PreProcessing/Head_Rotation_Mouse/B_group/Face_2W_B2_S1.csv',\n",
    "    f'D:/MultiModal/Data/Data_PreProcessing/Head_Rotation_Mouse/B_group/Face_2W_B3_S1.csv',\n",
    "    f'D:/MultiModal/Data/Data_PreProcessing/Head_Rotation_Mouse/B_group/Face_2W_B4_S1.csv',\n",
    "]\n",
    "\n",
    "# Group time 시간을 자동으로 동그라미 칠 수 있도록 불러올 것. \n",
    "'''\n",
    "\n",
    "To do list) \n",
    "1. 그룹 시간을 저장한 파일 가져오기. \n",
    "2. 가져온 파일 안에서의 시간 정보를 바탕으로, Correlation 그래프에 표시하도록 할 것. \n",
    "\n",
    "'''\n",
    "\n",
    "data_xlse = []\n",
    "valid_files = True\n",
    "for file in csv_files:\n",
    "    if os.path.exists(file):\n",
    "        df = pd.read_csv(file)\n",
    "        # Check if the dataframe is empty or all values are NaN\n",
    "        if df.empty: # or df.isnull().all().all()\n",
    "            print(f\"File {file} is empty. Skipping this set.\")\n",
    "            valid_files = False\n",
    "            break\n",
    "        data_xlse.append(df)\n",
    "    else:\n",
    "        print(f\"File {file} does not exist. Skipping this set.\")\n",
    "        valid_files = False\n",
    "        break\n",
    "\n",
    "if valid_files:\n",
    "    # Load data and extract the column of interest\n",
    "    data_xlse = [pd.read_csv(file) for file in csv_files]\n",
    "    data = [df['X'] for df in data_xlse] # X , Y, Z, Delta_X, Delta_Y, Delta_Z \n",
    "\n",
    "    # Define frame rate and time window\n",
    "    frame_rate = 25  # frames per second\n",
    "    start_time = 0  # in seconds\n",
    "    end_time =  1199 # in seconds\n",
    "\n",
    "    start_frame = start_time * frame_rate\n",
    "    end_frame = end_time * frame_rate\n",
    "\n",
    "    #start_frame = 0\n",
    "    #end_frame = 1200\n",
    "\n",
    "    # 샘플 신호 생성\n",
    "    fs = frame_rate  # 샘플링 주파수 (Hz)\n",
    "    num_samples = end_frame - start_frame\n",
    "    t = np.linspace(start_time, end_time, num_samples, endpoint=False)\n",
    "\n",
    "\n",
    "    # 로우패스 필터 적용\n",
    "    cutoff = 0.5  # 커트오프 주파수 (Hz)\n",
    "\n",
    "    '''\n",
    "    # Specify the bandpass filter parameters\n",
    "    lowcut = 0.1  # Low cutoff frequency in Hz\n",
    "    highcut = 0.5  # High cutoff frequency in Hz\n",
    "    fs = 25  # Sampling frequency in Hz\n",
    "    order = 5  # Filter order\n",
    "    '''\n",
    "\n",
    "    # 머리 움직임 값.\n",
    "    signal1_raw = data[0][start_frame:end_frame].to_numpy()\n",
    "    signal2_raw = data[1][start_frame:end_frame].to_numpy()\n",
    "\n",
    "    signal1_raw = zscore_signal(signal1_raw)\n",
    "    signal2_raw = zscore_signal(signal2_raw)\n",
    "\n",
    "    # Extract data within the specified window\n",
    "    signal1 = lowpass_filter(signal1_raw, cutoff, fs) # 변경 전= signal1_raw, cutoff, fs\n",
    "    signal2 = lowpass_filter(signal2_raw, cutoff, fs) # 변경 전, signal2_raw, cutoff, fs\n",
    "\n",
    "    signal1 = zscore_signal(signal1)\n",
    "    signal2 = zscore_signal(signal2)\n",
    "    \n",
    "    # Directional Agreement를 계산합니다.\n",
    "    #da_score = directional_agreement(signal1, signal2)\n",
    "\n",
    "    # Cross Correlation 계산\n",
    "    #cross_corr_values, time_lags = cross_correlation(signal1, signal2)\n",
    "\n",
    "    # Calculate Pearson correlation coefficients for raw and filtered signals at each time lag\n",
    "    #cross_corr_values_raw, time_lags_raw = cross_correlation(signal1_raw, signal2_raw)\n",
    "    #cross_corr_values_filtered, time_lags_filtered = cross_correlation(signal1, signal2)\n",
    "\n",
    "    # Calculate rolling window correlation\n",
    "    window_size = 60 * frame_rate\n",
    "    rolling_correlations_raw = rolling_window_correlation(signal1_raw, signal2_raw, window_size)\n",
    "    #rolling_correlations_filtered = rolling_window_correlation(signal1, signal2, window_size)\n",
    "    \n",
    "    # Generate the time index for rolling correlations\n",
    "    rolling_time_index = np.arange(len(rolling_correlations_raw))\n",
    "\n",
    "    # Convert frame indices to time in seconds\n",
    "    time_seconds = np.linspace(start_time, end_time, len(signal1_raw))\n",
    "    \n",
    "    # Plot additional Rolling Window Correlation alongside existing plots\n",
    "    plt.figure(figsize=(12, 10))\n",
    "\n",
    "    # Plot 1: Signals (Raw and Filtered)\n",
    "    #plt.subplot(3, 1, 1)\n",
    "    #plt.plot(time_seconds,signal1_raw, label='Signal 1 Raw', color='blue')\n",
    "    #plt.plot(time_seconds,signal2_raw, label='Signal 2 Raw', color='orange')\n",
    "    #plt.plot(time_seconds,signal1, label='Signal 1 Filtered', color='green')\n",
    "    #plt.plot(time_seconds,signal2, label='Signal 2 Filtered', color='red')\n",
    "    #plt.xlabel('Time (seconds)')\n",
    "    #plt.ylabel('Amplitude')\n",
    "    #plt.title('Raw and Filtered Signals')\n",
    "    #plt.legend()\n",
    "\n",
    "    avg_rolling_correlation_raw = np.mean(rolling_correlations_raw)\n",
    "    #avg_rolling_correlation_filtered = np.mean(rolling_correlations_filtered)\n",
    "\n",
    "    # Plot 2: Rolling Window Correlation with average values in the title\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(time_seconds[window_size // 2:len(rolling_correlations_raw) + window_size // 2], rolling_correlations_raw, label='Rolling Raw', color='blue')\n",
    "    #plt.plot(time_seconds[window_size // 2:len(rolling_correlations_filtered) + window_size // 2], rolling_correlations_filtered, label='Rolling Filtered', color='green')\n",
    "    plt.xlabel('Time (seconds)')\n",
    "    plt.ylabel('Rolling Window Pearson Correlation')\n",
    "    #plt.ylim(-1.0, 1.0)\n",
    "    plt.title(f'Rolling Window Pearson Correlation\\nAvg Raw Correlation: {avg_rolling_correlation_raw:.2f}') # \\nAvg Filtered Correlation: {avg_rolling_correlation_filtered:.2f}'\n",
    "    max_time = max(time_seconds)\n",
    "    x_ticks = range(0, int(max_time) + 60, 60)\n",
    "    plt.xticks(x_ticks)\n",
    "    plt.legend()\n",
    "\n",
    "    # Plotting group times as circles\n",
    "    # for gt in group_times_seconds:\n",
    "    #     plt.axvline(x=gt, color='red', linestyle='--', linewidth=1, label='Group Time' if gt == group_times_seconds.iloc[0] else \"\")\n",
    "        \n",
    "    for gt in group_times_seconds_adjusted:\n",
    "        plt.axvline(x=gt, color='red', linestyle='--', linewidth=1, label='2명 Group Time' if gt == group_times_seconds_adjusted.iloc[0] else \"\")\n",
    "\n",
    "    for gt in group_times_seconds_adjusted_3:\n",
    "         plt.axvline(x=gt, color='green', linestyle='--', linewidth=1, label='3명 Group Time' if gt == group_times_seconds_adjusted_3.iloc[0] else \"\")\n",
    "    \n",
    "\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    img_path = 'D:/MultiModal/MultiModal_Model/Head_Rotation_Mouse/Synchrony_Plot/'\n",
    "    plt.savefig(img_path + f'B_group_2W_S1.png')\n",
    "else:\n",
    "    print(\"Vaild data files not found. Processing skipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.signal import butter, filtfilt\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def directional_agreement(signal1, signal2):\n",
    "    signal1_norm = (signal1 - np.mean(signal1)) / np.std(signal1)\n",
    "    signal2_norm = (signal2 - np.mean(signal2)) / np.std(signal2)\n",
    "    dot_product = np.dot(signal1_norm, signal2_norm)\n",
    "    directional_agreement = dot_product / len(signal1)\n",
    "    return directional_agreement\n",
    "\n",
    "def butter_lowpass(cutoff, fs, order=5):\n",
    "    nyquist = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyquist\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    return b, a\n",
    "\n",
    "def lowpass_filter(data, cutoff, fs, order=5):\n",
    "    b, a = butter_lowpass(cutoff, fs, order=order)\n",
    "    y = filtfilt(b, a, data)\n",
    "    return y\n",
    "\n",
    "def cross_correlation(signal1, signal2):\n",
    "    time_lags = np.arange(-len(signal1) + 1, len(signal1))\n",
    "    cross_corr = [np.correlate(signal1, np.roll(signal2, shift), mode='valid')[0] for shift in time_lags]\n",
    "    return np.array(cross_corr), time_lags\n",
    "\n",
    "def pearson_cross_correlation(signal1, signal2):\n",
    "    signal1_mean = np.mean(signal1)\n",
    "    signal2_mean = np.mean(signal2)\n",
    "    signal1_adjusted = signal1 - signal1_mean\n",
    "    signal2_adjusted = signal2 - signal2_mean\n",
    "    numerator = np.sum(signal1_adjusted * signal2_adjusted)\n",
    "    denominator = np.sqrt(np.sum(signal1_adjusted ** 2) * np.sum(signal2_adjusted ** 2))\n",
    "    pearson_corr = numerator / denominator\n",
    "    return pearson_corr\n",
    "\n",
    "def rolling_window_correlation(signal1, signal2, window_size):\n",
    "    num_samples = len(signal1)\n",
    "    correlations = []\n",
    "    for i in tqdm(range(num_samples - window_size + 1), desc=\"Calculating Rolling Window Correlation\"):\n",
    "        window1 = signal1[i : i + window_size]\n",
    "        window2 = signal2[i : i + window_size]\n",
    "        correlation = np.corrcoef(window1, window2)[0, 1]\n",
    "        correlations.append(correlation)\n",
    "    return np.array(correlations)\n",
    "\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band', analog=False)\n",
    "    return b, a\n",
    "\n",
    "def zscore_signal(signal):\n",
    "    signal_mean = np.mean(signal)\n",
    "    signal_std = np.std(signal)\n",
    "    if signal_std == 0:\n",
    "        signal_normalized = np.zeros_like(signal)\n",
    "    else:\n",
    "        signal_normalized = (signal - signal_mean) / signal_std\n",
    "    return signal_normalized\n",
    "\n",
    "# Load data from CSV files || 입력받으려는 데이터의 경우, Head Rotation 의 (x,y,z 값과 각 축의 변화량 값, lip_distance 값)이 있는 것. \n",
    "# 얼굴 움직임 값을 계산하기 위해서 불러오기 위한 코드 부분. \n",
    "\n",
    "# Load the uploaded Excel file\n",
    "# file_path = 'C:/Users/user/Desktop/Group_Lean_time_2.xlsx'\n",
    "# xls = pd.ExcelFile(file_path)\n",
    "\n",
    "# Load the 'A_group' sheet into a DataFrame\n",
    "# a_group_df = pd.read_excel(file_path, sheet_name='A_group_2W_S2')\n",
    "\n",
    "# Extracting the time information from the relevant column\n",
    "# time_data = a_group_df['고개 위아래 끄덕임 (2명)'][1:]  # Skipping the first two rows which are headers\n",
    "# print(time_data)\n",
    "# # NaN 값이 있는 구간은 없애주기.\n",
    "# time_data = time_data.dropna()\n",
    "# #print(time_data)\n",
    "# time_data = time_data.astype(str)\n",
    "\n",
    "# Converting time data to seconds\n",
    "# def time_to_seconds(time_str):\n",
    "#     h, m, s = map(int, time_str.split(':'))\n",
    "#     # 시간, 분, 초 \n",
    "#     total_seconds = h * 3600 + m * 60 + s\n",
    "    \n",
    "#     #return h * 3600 + m * 60 + s\n",
    "#     # 전체 시간에서 20분을 뻄. \n",
    "#     if total_seconds > 1200:  # Only subtract 20 minutes if total time is more than 20 minutes\n",
    "#         adjusted_seconds = total_seconds - 1200\n",
    "#     else:\n",
    "#         adjusted_seconds = total_seconds\n",
    "#     return adjusted_seconds\n",
    "\n",
    "# group_times_seconds_adjusted = time_data.apply(time_to_seconds).reset_index(drop=True)\n",
    "# #group_times_seconds = time_data.apply(time_to_seconds).reset_index(drop=True)\n",
    "# print(group_times_seconds_adjusted)\n",
    "\n",
    "\n",
    "# Extracting the time information from the '고개 위아래 끄덕임 (3명)' column\n",
    "# time_data_s2_3 = a_group_df['고개 위아래 끄덕임 (3명)'][1:]  # Skipping the first two rows which are headers\n",
    "# time_data_s2_3 = time_data_s2_3.dropna()\n",
    "# time_data_s2_3 = time_data_s2_3.astype(str)\n",
    "\n",
    "# Converting time data to seconds and subtracting 20 minutes for '고개 위아래 끄덕임 (3명)'\n",
    "# group_times_seconds_adjusted_3 = time_data_s2_3.apply(time_to_seconds).reset_index(drop=True)\n",
    "# print(group_times_seconds_adjusted_3)\n",
    "\n",
    "csv_files = [\n",
    "    f'D:/MultiModal/Data/Data_PreProcessing/Head_Rotation_Mouse/G_group/Face_2W_G1_S1.csv',\n",
    "    f'D:/MultiModal/Data/Data_PreProcessing/Head_Rotation_Mouse/G_group/Face_2W_G2_S1.csv',\n",
    "    f'D:/MultiModal/Data/Data_PreProcessing/Head_Rotation_Mouse/G_group/Face_2W_G3_S1.csv',\n",
    "    f'D:/MultiModal/Data/Data_PreProcessing/Head_Rotation_Mouse/G_group/Face_2W_G4_S1.csv',\n",
    "]\n",
    "\n",
    "# Group time 시간을 자동으로 동그라미 칠 수 있도록 불러올 것. \n",
    "'''\n",
    "\n",
    "To do list) \n",
    "1. 그룹 시간을 저장한 파일 가져오기. \n",
    "2. 가져온 파일 안에서의 시간 정보를 바탕으로, Correlation 그래프에 표시하도록 할 것. \n",
    "\n",
    "'''\n",
    "\n",
    "data_xlse = []\n",
    "valid_files = True\n",
    "for file in csv_files:\n",
    "    if os.path.exists(file):\n",
    "        df = pd.read_csv(file)\n",
    "        # Check if the dataframe is empty or all values are NaN\n",
    "        if df.empty: # or df.isnull().all().all()\n",
    "            print(f\"File {file} is empty. Skipping this set.\")\n",
    "            valid_files = False\n",
    "            break\n",
    "        data_xlse.append(df)\n",
    "    else:\n",
    "        print(f\"File {file} does not exist. Skipping this set.\")\n",
    "        valid_files = False\n",
    "        break\n",
    "\n",
    "if valid_files:\n",
    "    # Load data and extract the column of interest\n",
    "    data_xlse = [pd.read_csv(file) for file in csv_files]\n",
    "    data = [df['X'] for df in data_xlse] # X , Y, Z, Delta_X, Delta_Y, Delta_Z \n",
    "\n",
    "    # Define frame rate and time window\n",
    "    frame_rate = 25  # frames per second\n",
    "    start_time = 0  # in seconds\n",
    "    end_time =  1199 # in seconds\n",
    "\n",
    "    start_frame = start_time * frame_rate\n",
    "    end_frame = end_time * frame_rate\n",
    "\n",
    "    #start_frame = 0\n",
    "    #end_frame = 1200\n",
    "\n",
    "    # 샘플 신호 생성\n",
    "    fs = frame_rate  # 샘플링 주파수 (Hz)\n",
    "    num_samples = end_frame - start_frame\n",
    "    t = np.linspace(start_time, end_time, num_samples, endpoint=False)\n",
    "\n",
    "    # 로우패스 필터 적용\n",
    "    cutoff = 0.5  # 커트오프 주파수 (Hz)\n",
    "\n",
    "    '''\n",
    "    # Specify the bandpass filter parameters\n",
    "    lowcut = 0.1  # Low cutoff frequency in Hz\n",
    "    highcut = 0.5  # High cutoff frequency in Hz\n",
    "    fs = 25  # Sampling frequency in Hz\n",
    "    order = 5  # Filter order\n",
    "    '''\n",
    "\n",
    "    # 머리 움직임 값.\n",
    "    signal1_raw = data[0][start_frame:end_frame].to_numpy()\n",
    "    signal2_raw = data[1][start_frame:end_frame].to_numpy()\n",
    "\n",
    "    signal1_raw = zscore_signal(signal1_raw)\n",
    "    signal2_raw = zscore_signal(signal2_raw)\n",
    "\n",
    "    # Extract data within the specified window\n",
    "    signal1 = lowpass_filter(signal1_raw, cutoff, fs) # 변경 전= signal1_raw, cutoff, fs\n",
    "    signal2 = lowpass_filter(signal2_raw, cutoff, fs) # 변경 전, signal2_raw, cutoff, fs\n",
    "\n",
    "    signal1 = zscore_signal(signal1)\n",
    "    signal2 = zscore_signal(signal2)\n",
    "    \n",
    "    # Directional Agreement를 계산합니다.\n",
    "    #da_score = directional_agreement(signal1, signal2)\n",
    "\n",
    "    # Cross Correlation 계산\n",
    "    #cross_corr_values, time_lags = cross_correlation(signal1, signal2)\n",
    "\n",
    "    # Calculate Pearson correlation coefficients for raw and filtered signals at each time lag\n",
    "    #cross_corr_values_raw, time_lags_raw = cross_correlation(signal1_raw, signal2_raw)\n",
    "    #cross_corr_values_filtered, time_lags_filtered = cross_correlation(signal1, signal2)\n",
    "\n",
    "    # Calculate rolling window correlation\n",
    "    window_size = 60 * frame_rate\n",
    "    rolling_correlations_raw = rolling_window_correlation(signal1_raw, signal2_raw, window_size)\n",
    "    #rolling_correlations_filtered = rolling_window_correlation(signal1, signal2, window_size)\n",
    "    \n",
    "    # Generate the time index for rolling correlations\n",
    "    rolling_time_index = np.arange(len(rolling_correlations_raw))\n",
    "\n",
    "    # Convert frame indices to time in seconds\n",
    "    time_seconds = np.linspace(start_time, end_time, len(signal1_raw))\n",
    "    \n",
    "    # Plot additional Rolling Window Correlation alongside existing plots\n",
    "    plt.figure(figsize=(12, 10))\n",
    "\n",
    "    # Plot 1: Signals (Raw and Filtered)\n",
    "    #plt.subplot(3, 1, 1)\n",
    "    #plt.plot(time_seconds,signal1_raw, label='Signal 1 Raw', color='blue')\n",
    "    #plt.plot(time_seconds,signal2_raw, label='Signal 2 Raw', color='orange')\n",
    "    #plt.plot(time_seconds,signal1, label='Signal 1 Filtered', color='green')\n",
    "    #plt.plot(time_seconds,signal2, label='Signal 2 Filtered', color='red')\n",
    "    #plt.xlabel('Time (seconds)')\n",
    "    #plt.ylabel('Amplitude')\n",
    "    #plt.title('Raw and Filtered Signals')\n",
    "    #plt.legend()\n",
    "\n",
    "    avg_rolling_correlation_raw = np.mean(rolling_correlations_raw)\n",
    "    #avg_rolling_correlation_filtered = np.mean(rolling_correlations_filtered)\n",
    "\n",
    "    # Plot 2: Rolling Window Correlation with average values in the title\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(time_seconds[window_size // 2:len(rolling_correlations_raw) + window_size // 2], rolling_correlations_raw, label='Rolling Raw', color='blue')\n",
    "    #plt.plot(time_seconds[window_size // 2:len(rolling_correlations_filtered) + window_size // 2], rolling_correlations_filtered, label='Rolling Filtered', color='green')\n",
    "    plt.xlabel('Time (seconds)')\n",
    "    plt.ylabel('Rolling Window Pearson Correlation')\n",
    "    #plt.ylim(-1.0, 1.0)\n",
    "    plt.title(f'Rolling Window Pearson Correlation\\nAvg Raw Correlation: {avg_rolling_correlation_raw:.2f}') # \\nAvg Filtered Correlation: {avg_rolling_correlation_filtered:.2f}'\n",
    "    max_time = max(time_seconds)\n",
    "    x_ticks = range(0, int(max_time) + 60, 60)\n",
    "    plt.xticks(x_ticks)\n",
    "    plt.legend()\n",
    "\n",
    "        \n",
    "    # for gt in group_times_seconds_adjusted:\n",
    "    #     plt.axvline(x=gt, color='red', linestyle='--', linewidth=1, label='2명 Group Time' if gt == group_times_seconds_adjusted.iloc[0] else \"\")\n",
    "\n",
    "    # for gt in group_times_seconds_adjusted_3:\n",
    "    #      plt.axvline(x=gt, color='green', linestyle='--', linewidth=1, label='3명 Group Time' if gt == group_times_seconds_adjusted_3.iloc[0] else \"\")\n",
    "    \n",
    "\n",
    "    plt.tight_layout()\n",
    "    #plt.show()\n",
    "    img_path = 'D:/MultiModal/MultiModal_Model/Head_Rotation_Mouse/Synchrony_Plot/'\n",
    "    plt.savefig(img_path + f'G_group_2W_S1.png')\n",
    "else:\n",
    "    print(\"Vaild data files not found. Processing skipped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corrleation 값을 추출하는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_window_correlation(signal1, signal2, window_size):\n",
    "    num_samples = len(signal1)\n",
    "    correlations = []\n",
    "    \n",
    "    for i in range(num_samples - window_size + 1):\n",
    "        window1 = signal1[i : i + window_size]\n",
    "        window2 = signal2[i : i + window_size]\n",
    "        \n",
    "        # Calculate Pearson correlation coefficient for the current window\n",
    "        correlation = np.corrcoef(window1, window2)[0, 1]\n",
    "        correlations.append(correlation)\n",
    "    \n",
    "    return np.array(correlations)\n",
    "\n",
    "def zscore_signal(signal):\n",
    "    signal_mean = np.mean(signal)\n",
    "    signal_std = np.std(signal)\n",
    "    \n",
    "     # Avoid division by zero\n",
    "    if signal_std == 0:\n",
    "        signal_normalized = np.zeros_like(signal)\n",
    "    else:\n",
    "        signal_normalized = (signal - signal_mean) / signal_std\n",
    "    \n",
    "    return signal_normalized\n",
    "    \n",
    "    # # Subtract the mean and divide by the standard deviation\n",
    "    # signal_normalized = (signal - signal_mean) / signal_std\n",
    "    \n",
    "    # return signal_normalized\n",
    "    \n",
    "def truncate_signals(signal1, signal2):\n",
    "    min_length = min(len(signal1), len(signal2))\n",
    "    return signal1[:min_length], signal2[:min_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_name = ['A','B','C','D','E','F','G']  \n",
    "weeks = ['1W','2W', '3W', '4W']\n",
    "section_num = ['S1','S2'] \n",
    "\n",
    "# Load data from CSV files\n",
    "total_path = 'D:/MultiModal/MultiModal_Model/Head_Rotation_Mouse/face_Synchrony/total_synchrony(delta).csv'\n",
    "total_synchrony = pd.DataFrame()\n",
    "\n",
    "for group in tqdm(group_name, desc = \"Groups\"): # desc: 진행 바 앞에 문자열을 출력하기 위해 쓰는 키워드 \n",
    "    for week in tqdm(weeks, desc=f\"Weeks for group {group}\"):\n",
    "        for section in tqdm(section_num, desc=f\"section for week {week}\"):\n",
    "            csv_files = [\n",
    "                f'D:/MultiModal/Data/Data_PreProcessing/Head_Rotation_Mouse/{group}_group_delta/Face_{week}_{group}1_{section}.csv',\n",
    "                f'D:/MultiModal/Data/Data_PreProcessing/Head_Rotation_Mouse/{group}_group_delta/Face_{week}_{group}2_{section}.csv',\n",
    "                f'D:/MultiModal/Data/Data_PreProcessing/Head_Rotation_Mouse/{group}_group_delta/Face_{week}_{group}3_{section}.csv',\n",
    "                f'D:/MultiModal/Data/Data_PreProcessing/Head_Rotation_Mouse/{group}_group_delta/Face_{week}_{group}4_{section}.csv'\n",
    "            ]\n",
    "            \n",
    "            data_xlse = [] \n",
    "            vaild_files = True # 파일 안에 데이터가 있는지 없는지\n",
    "            for file in csv_files:\n",
    "                if os.path.exists(file): # csv_files[0]\n",
    "                    df = pd.read_csv(file)\n",
    "                    if df.empty:\n",
    "                        print(f\"File {file} is empty. Skip this set\")\n",
    "                        vaild_files = False\n",
    "                    data_xlse.append(df)\n",
    "                else:\n",
    "                    print(f\"File {file} does not exist. Skip this set\")\n",
    "                    vaild_files = False\n",
    "                    break\n",
    "                \n",
    "            if vaild_files:\n",
    "                # 길이가 맞지 않는 데이터의 경우 넘어갈 수 있도록 설정. \n",
    "                lengths = [len(df) for df in data_xlse]\n",
    "                #print(lengths)\n",
    "                if len(set(lengths)) != 1:\n",
    "                    print(f\"Files for group {group}, week {week}, section {section} have different lengths. Skipping this set.\")\n",
    "                    continue\n",
    "                # 추출하고 싶은 데이터 값을 넣어 둠. \n",
    "                data_xlse = [pd.read_csv(file) for file in csv_files]\n",
    "                \n",
    "                # 머리 움직임 값을 계산해서, 저장하고 싶은데 어떻게 해야할까? \n",
    "                data = [np.sqrt(df['X']**2 + df['Y']**2 + df['Z']**2) for df in data_xlse] # X, Y, Z, Delta_X, Delta_Y, Delta_Z, Lip_Distance\n",
    "                #data = [np.sqrt(df['Delta_X']**2 + df['Delta_Y']**2 + df['Delta_Z']**2) for df in data_xlse] \n",
    "                \n",
    "                frame_rate = 25\n",
    "                data_frame_section = pd.DataFrame() # 네 개의 신호 간 모든 쌍에 대해 상관 관계를 계산하여 section_means 값에 저장. \n",
    "                section_means = []\n",
    "                \n",
    "                for i in range(4):\n",
    "                    for j in range(i+1, 4):\n",
    "                        signal1 = np.array(data[i]) # 머리 움직임에서의 값...\n",
    "                        #print(\"signal1 : \", signal1)\n",
    "                        signal2 = np.array(data[j])\n",
    "                        #print(\"signal2 : \", signal2)\n",
    "                        \n",
    "                        signal1, signal2 = truncate_signals(signal1, signal2)\n",
    "                        \n",
    "                        signal1 = zscore_signal(signal1)\n",
    "                        signal2 = zscore_signal(signal2)\n",
    "                        \n",
    "                        window_size = 60 * frame_rate\n",
    "                        corr_data = rolling_window_correlation(signal1, signal2, window_size)\n",
    "                        section_means.append(np.mean(corr_data))\n",
    "                        col_name = f\"{i}_{j}\"\n",
    "                        data_frame_section[col_name] = corr_data\n",
    "                \n",
    "                total_synchrony[f'{group}_{week}_{section}'] = section_means\n",
    "                excel_path = f'D:/MultiModal/MultiModal_Model/Head_Rotation_Mouse/face_Synchrony/face_movement/Face_{week}_{group}_{section}_movement_Synchrony.csv'\n",
    "                data_frame_section.to_csv(excel_path, index=False)\n",
    "\n",
    "total_synchrony.to_csv(total_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 몸의 sync 값을 출력하는 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_name = ['A','B','C','D','E','F','G']  \n",
    "weeks = ['1W','2W', '3W', '4W']\n",
    "section_num = ['S1','S2'] \n",
    "\n",
    "# Load data from CSV files\n",
    "total_path = 'D:/MultiModal/MultiModal_Model/Head_Rotation_Mouse/face_Synchrony/total_body_synchrony.csv'\n",
    "total_synchrony = pd.DataFrame()\n",
    "\n",
    "for group in tqdm(group_name, desc = \"Groups\"): # desc: 진행 바 앞에 문자열을 출력하기 위해 쓰는 키워드 \n",
    "    for week in tqdm(weeks, desc=f\"Weeks for group {group}\"):\n",
    "        for section in tqdm(section_num, desc=f\"section for week {week}\"):\n",
    "            csv_files = [\n",
    "                f'D:/MultiModal/Data/Data_PreProcessing/Head_Rotation_Mouse/{group}_group_delta/Face_{week}_{group}1_{section}.csv',\n",
    "                f'D:/MultiModal/Data/Data_PreProcessing/Head_Rotation_Mouse/{group}_group_delta/Face_{week}_{group}2_{section}.csv',\n",
    "                f'D:/MultiModal/Data/Data_PreProcessing/Head_Rotation_Mouse/{group}_group_delta/Face_{week}_{group}3_{section}.csv',\n",
    "                f'D:/MultiModal/Data/Data_PreProcessing/Head_Rotation_Mouse/{group}_group_delta/Face_{week}_{group}4_{section}.csv'\n",
    "            ]\n",
    "            \n",
    "            data_xlse = [] \n",
    "            vaild_files = True # 파일 안에 데이터가 있는지 없는지\n",
    "            for file in csv_files:\n",
    "                if os.path.exists(file): # csv_files[0]\n",
    "                    df = pd.read_csv(file)\n",
    "                    if df.empty:\n",
    "                        print(f\"File {file} is empty. Skip this set\")\n",
    "                        vaild_files = False\n",
    "                    data_xlse.append(df)\n",
    "                else:\n",
    "                    print(f\"File {file} does not exist. Skip this set\")\n",
    "                    vaild_files = False\n",
    "                    break\n",
    "                \n",
    "            if vaild_files:\n",
    "                # 길이가 맞지 않는 데이터의 경우 넘어갈 수 있도록 설정. \n",
    "                lengths = [len(df) for df in data_xlse]\n",
    "                #print(lengths)\n",
    "                if len(set(lengths)) != 1:\n",
    "                    print(f\"Files for group {group}, week {week}, section {section} have different lengths. Skipping this set.\")\n",
    "                    continue\n",
    "                # 추출하고 싶은 데이터 값을 넣어 둠. \n",
    "                data_xlse = [pd.read_csv(file) for file in csv_files]\n",
    "                \n",
    "                # 머리 움직임 값을 계산해서, 저장하고 싶은데 어떻게 해야할까? \n",
    "                data = [np.sqrt(df['X']**2 + df['Y']**2 + df['Z']**2) for df in data_xlse] # X, Y, Z, Delta_X, Delta_Y, Delta_Z, Lip_Distance\n",
    "                #data = [np.sqrt(df['Delta_X']**2 + df['Delta_Y']**2 + df['Delta_Z']**2) for df in data_xlse] \n",
    "                \n",
    "                frame_rate = 25\n",
    "                data_frame_section = pd.DataFrame() # 네 개의 신호 간 모든 쌍에 대해 상관 관계를 계산하여 section_means 값에 저장. \n",
    "                section_means = []\n",
    "                \n",
    "                for i in range(4):\n",
    "                    for j in range(i+1, 4):\n",
    "                        signal1 = np.array(data[i]) # 머리 움직임에서의 값...\n",
    "                        #print(\"signal1 : \", signal1)\n",
    "                        signal2 = np.array(data[j])\n",
    "                        #print(\"signal2 : \", signal2)\n",
    "                        \n",
    "                        signal1, signal2 = truncate_signals(signal1, signal2)\n",
    "                        \n",
    "                        signal1 = zscore_signal(signal1)\n",
    "                        signal2 = zscore_signal(signal2)\n",
    "                        \n",
    "                        window_size = 60 * frame_rate\n",
    "                        corr_data = rolling_window_correlation(signal1, signal2, window_size)\n",
    "                        section_means.append(np.mean(corr_data))\n",
    "                        col_name = f\"{i}_{j}\"\n",
    "                        data_frame_section[col_name] = corr_data\n",
    "                \n",
    "                total_synchrony[f'{group}_{week}_{section}'] = section_means\n",
    "                excel_path = f'D:/MultiModal/MultiModal_Model/Head_Rotation_Mouse/face_Synchrony/face_movement/Face_{week}_{group}_{section}_movement_Synchrony.csv'\n",
    "                data_frame_section.to_csv(excel_path, index=False)\n",
    "\n",
    "total_synchrony.to_csv(total_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파일이 섞여서 저장이 되었으므로, 이를 분류하여 따로 저장할 수 있는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# 현재 파일들이 섞여 있는 폴더 경로와 이동할 폴더 경로 설정\n",
    "current_folder = 'D:/MultiModal/MultiModal_Model/Head_Rotation_Mouse/face_Synchrony/Save_File_delta/'\n",
    "destination_folder = 'D:/MultiModal/MultiModal_Model/Head_Rotation_Mouse/face_Synchrony/face_movement/'\n",
    "\n",
    "# Temp_folder가 존재하지 않으면 생성\n",
    "if not os.path.exists(destination_folder):\n",
    "    os.makedirs(destination_folder)\n",
    "\n",
    "# 현재 폴더 내 파일들을 확인하고 이동\n",
    "for filename in os.listdir(current_folder):\n",
    "    if 'movement_Synchrony' in filename:\n",
    "        # 이동할 파일의 전체 경로 생성\n",
    "        src_file_path = os.path.join(current_folder, filename)\n",
    "        dest_file_path = os.path.join(destination_folder, filename)\n",
    "        \n",
    "        # 파일 이동\n",
    "        shutil.move(src_file_path, dest_file_path)\n",
    "        print(f\"Moved: {filename}\")\n",
    "\n",
    "print(\"Files have been moved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synchrony 값이 저장된 파일을 불러오고, 주차 별로 평균내는 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로에 존재하는 파일 읽어오기. \n",
    "Weeks = ['1W', '2W', '3W', '4W']\n",
    "Steps = ['S1', 'S2']\n",
    "groups = ['A', 'B', 'C', 'D', 'E', 'F', 'G']\n",
    "\n",
    "path = 'D:/MultiModal/MultiModal_Model/Head_Rotation_Mouse/face_Synchrony/face_movement/'\n",
    "\n",
    "#그룹 별 평균 값을 저장하기 위한 딕셔너리 선언 \n",
    "group_means = {group: {week: [] for week in Weeks} for group in groups}\n",
    "\n",
    "for group in groups:\n",
    "    for week in Weeks:\n",
    "        for step in Steps:\n",
    "            # Set the directory path\n",
    "            directory_path = os.path.join(path)\n",
    "            \n",
    "            # Get the list of files in the directory\n",
    "            if os.path.exists(directory_path):\n",
    "                file_list = os.listdir(directory_path)\n",
    "                \n",
    "                for file_name in file_list:\n",
    "                    # Check if the file name contains the specific week, group, and step\n",
    "                    if f'Face_{week}_{group}_{step}_movement_Synchrony' in file_name:\n",
    "                        # Set the file path\n",
    "                        file_path = os.path.join(directory_path, file_name)\n",
    "                        read_test = pd.read_csv(file_path)\n",
    "                        \n",
    "                        #print('Each column names in csv : ', read_test.columns)\n",
    "                        \n",
    "                        # 각 신호의 평균 값을 저장하기 위한 곳. \n",
    "                        columns_of_interest = ['0_1', '0_2', '0_3', '1_2', '1_3', '2_3']\n",
    "                        col_means = read_test[columns_of_interest].mean(axis=0).mean()\n",
    "                        \n",
    "                        #print(f\"Current {week}_{group}_{step} mean : \", col_means)\n",
    "                        \n",
    "                        group_means[group][week].append(col_means)\n",
    "                        #rint('Current group means : ', group_means )\n",
    "\n",
    "# #NaN 값이 있을 시, 평균 값이 계산되지 않으므로, nan 부분이 있다면 넘어갈 수 있도록 세팅.\n",
    "weekly_means = {group: {week: np.nanmean(values) if len(values) > 0 else np.nan for week, values in group_means[group].items()} for group in groups}\n",
    "\n",
    "weekly_means_df = pd.DataFrame(weekly_means).T\n",
    "# 그룹의 상관관계가 있는 값을 가지고, 보는 것.\n",
    "output_path = 'D:/MultiModal/Data/Data_PreProcessing/Head_Rotation_Mouse/group_weekly_synchrony_means.xlsx'\n",
    "weekly_means_df.to_excel(output_path)\n",
    "print(weekly_means_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
